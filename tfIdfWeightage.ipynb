{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://storage.googleapis.com/knowledge-base-data/sre01.txt', 'SRE: The Saga Of The Best SRE Game Ever Played! By Josh Renaud'), ('https://storage.googleapis.com/knowledge-base-data/srex.txt', 'Solar Realms Elite: X1 and X2, by Josh Renaud'), ('https://storage.googleapis.com/knowledge-base-data/100west.txt', 'Going 100 West by 53 North by Jim Prentice (1990)'), ('https://storage.googleapis.com/knowledge-base-data/20.lws', 'What Research and Development Was Always Meant to Be, by M. Peshota'), ('https://storage.googleapis.com/knowledge-base-data/3gables.txt', 'The Adventure of the Three Gables'), ('https://storage.googleapis.com/knowledge-base-data/3lpigs.txt', 'The Story of the 3 Little Pigs'), ('https://storage.googleapis.com/knowledge-base-data/3sonnets.vrs', 'A Collection of Sonnets by Staeorra Rokraven (March 9, 1989)'), ('https://storage.googleapis.com/knowledge-base-data/3student.txt', 'The Adventure of the Three Students'), ('https://storage.googleapis.com/knowledge-base-data/3wishes.txt', 'The Story of the Three Wishes'), ('https://storage.googleapis.com/knowledge-base-data/4moons.txt', 'Fiction: The 4 Moons of Death'), ('https://storage.googleapis.com/knowledge-base-data/5orange.txt', 'The Story of the Five Orange Pips'), ('https://storage.googleapis.com/knowledge-base-data/6ablemen.txt', 'The Story of the Six Able Men'), ('https://storage.googleapis.com/knowledge-base-data/6napolen.txt', 'The Adventure of the Six Napoleons'), ('https://storage.googleapis.com/knowledge-base-data/7oldsamr.txt', 'The Seven Old Samurai'), ('https://storage.googleapis.com/knowledge-base-data/7voysinb.txt', 'The Seven Voyages of Sinbad the Sailor'), ('https://storage.googleapis.com/knowledge-base-data/ab40thv.txt', 'The Story of Ali Baba and the 40 Thieves'), ('https://storage.googleapis.com/knowledge-base-data/abbey.txt', 'The Adventure of the Abbey Grange (RP Story)'), ('https://storage.googleapis.com/knowledge-base-data/abyss.txt', 'How life goes on at the edge of Abyss'), ('https://storage.googleapis.com/knowledge-base-data/adler.txt', 'The Chronicles of Astrus II: Father Adler by Ben Blumenberg'), ('https://storage.googleapis.com/knowledge-base-data/adv_alad.txt', 'The Adventures of Aladdin'), ('https://storage.googleapis.com/knowledge-base-data/advsayed.txt', \"Sayed's Adventures\"), ('https://storage.googleapis.com/knowledge-base-data/advtthum.txt', 'The Adventures of Tom Thumb'), ('https://storage.googleapis.com/knowledge-base-data/aesop11.txt', \"Aesop's Fables Translated by George Fyler Townsend\"), ('https://storage.googleapis.com/knowledge-base-data/aesopa10.txt', \"Aesop's Fables (84 of Them) from The PaperLess Readers Club\"), ('https://storage.googleapis.com/knowledge-base-data/aircon.txt', 'The Air Conditioning Story'), ('https://storage.googleapis.com/knowledge-base-data/aisle.six', 'Aisle Six, by Morpheous of LOM'), ('https://storage.googleapis.com/knowledge-base-data/aislesix.txt', 'Aisle Six, by Morpheous, an LOM Release'), ('https://storage.googleapis.com/knowledge-base-data/alad10.txt', 'Aladdin and the Wonderful Lamp'), ('https://storage.googleapis.com/knowledge-base-data/alissadl.txt', \"The Fable of Ali and the Sultan's Saddle\"), ('https://storage.googleapis.com/knowledge-base-data/altside.hum', 'Alternate Side of the Street; Screenplay by Jeff Massie'), ('https://storage.googleapis.com/knowledge-base-data/aluminum.hum', 'An Aluminum Can A Day, by Carl Mitchell'), ('https://storage.googleapis.com/knowledge-base-data/aminegg.txt', 'The Fable of Amin and the Eggs'), ('https://storage.googleapis.com/knowledge-base-data/angelfur.hum', \"Angel's Fur, by Piper Sickles\"), ('https://storage.googleapis.com/knowledge-base-data/angry_ca.txt', 'Cliff-Hanger, a Poem'), ('https://storage.googleapis.com/knowledge-base-data/antcrick.txt', 'The Fable of the Ant and the Cricket'), ('https://storage.googleapis.com/knowledge-base-data/aquith.txt', 'My Lord Aquith: Novella by Yves Barbero'), ('https://storage.googleapis.com/knowledge-base-data/arcadia.sty', 'Arcadia, a Story'), ('https://storage.googleapis.com/knowledge-base-data/archive', ' A Collection of Various Good Stories'), ('https://storage.googleapis.com/knowledge-base-data/arctic.txt', 'The Arctic Circle and Beyond, by Jim Prentice (1990)'), ('https://storage.googleapis.com/knowledge-base-data/asop', 'Updating Aesop, or The Old Lady and the Crypt or Blood'), ('https://storage.googleapis.com/knowledge-base-data/3arcingfault.txt', 'The Arcing Fault'), ('https://storage.googleapis.com/knowledge-base-data/shortCircuit.txt', 'Difference Between Short Circuit & Overload Fault'), ('https://storage.googleapis.com/knowledge-base-data/phaseloss.txt', 'Phase Loss'), ('https://storage.googleapis.com/knowledge-base-data/poweroutage.txt', ''), ('https://storage.googleapis.com/knowledge-base-data/zombies.txt', 'Zombies by Suzanne L. McAllister (Spoof/Tribute of George Romero Living Dead Movies)')]\n",
      "len dataset: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dnilesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_vocab_size: 9668\n",
      "['sre', 'saga', 'best', 'game', 'ever', 'play', 'josh', 'renaud', 'bbse', 'call', 'solar', 'realm', 'elit', 'devolop', 'ibm', 'version', 'atari', 'see', 'realli', 'fun']\n",
      "tf_idf: 0.004844772152299886\n",
      "th_idf_title: 0.004844772152299886\n",
      "len tf_idf: 35330\n",
      "creating new instance\n",
      "connecting to broker\n",
      ".\n",
      "Subscribing to topic fault-extracted-data-events\n",
      ".\n",
      "Subscribing to topic fault-extracted-data-events\n",
      ".\n",
      "Message received\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "***** bot-extracted-fault-events ***** b'{\"topic\": \"bot-fault-events\", \"fault\": \"Short circuit Fault \\\\n\", \"deviceId\": \"BD Transport \\\\n\"}'\n",
      "BD Transport \n",
      " Short circuit Fault \n",
      " bot-fault-events\n",
      "Publishing message to topic chatbot-fault-events\n",
      "Matching Score\n",
      "\n",
      "Query: Short circuit Fault \n",
      "\n",
      "\n",
      "['short', 'circuit', 'fault']\n",
      "\n",
      "l: [41, 40, 42, 43, 3, 2, 14, 33, 18, 4]\n",
      "https://storage.googleapis.com/knowledge-base-data/3arcingfault.txt\n",
      "**s: https://storage.googleapis.com/knowledge-base-data/shortCircuit.txt,  https://storage.googleapis.com/knowledge-base-data/3arcingfault.txt, https://storage.googleapis.com/knowledge-base-data/phaseloss.txt\n",
      "first matched topic:\n",
      "%%% l is: https://storage.googleapis.com/knowledge-base-data/shortCircuit.txt,  https://storage.googleapis.com/knowledge-base-data/3arcingfault.txt, https://storage.googleapis.com/knowledge-base-data/phaseloss.txt\n",
      ".\n",
      "message published on bot-fault-events-sub\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6679d653fe49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;31m# Other loop*() functions are available that give a threaded interface and a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;31m# manual interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/paho/mqtt/client.py\u001b[0m in \u001b[0;36mloop_forever\u001b[0;34m(self, timeout, max_packets, retry_first_connection)\u001b[0m\n\u001b[1;32m   1576\u001b[0m             \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMQTT_ERR_SUCCESS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMQTT_ERR_SUCCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_packets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m                 \u001b[0;31m# We don't need to worry about locking here, because we've\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 \u001b[0;31m# either called loop_forever() when in single threaded mode, or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/paho/mqtt/client.py\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, timeout, max_packets)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sockpairR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0msocklist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;31m# Socket isn't correct type, in likelihood connection is lost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import urllib\n",
    "from io import open\n",
    "from importlib import reload\n",
    "import paho.mqtt.client as mqtt\n",
    "import paho.mqtt.subscribe as subscribe\n",
    "import time\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import import_ipynb\n",
    "from importlib import reload\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding('ISO-8859-1')\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# title = \"stories\"\n",
    "alpha = 0.3\n",
    "\n",
    "\n",
    "\n",
    "dataset = []\n",
    "\n",
    "c = False\n",
    "\n",
    "\n",
    "text = urllib.request.urlopen(\"https://storage.googleapis.com/knowledge-base-data/index.html\").read().decode(\n",
    "    'utf-8').strip()\n",
    "\n",
    "# file.close()\n",
    "file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "if c == False:\n",
    "    file_name = file_name[2:]\n",
    "    c = True\n",
    "\n",
    "for j in range(len(file_name)):\n",
    "    dataset.append((\"\" + str(file_name[j]), file_title[j]))\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "N = len(dataset)\n",
    "print(\"len dataset:\", N)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def print_doc(id):\n",
    "    print(\"dataset[id]:\", dataset[id])\n",
    "    text = urllib.request.urlopen(dataset[id][0]).read().decode('ISO-8859-1').strip()\n",
    "\n",
    "    # file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    # text = file.read().strip()\n",
    "    # file.close()\n",
    "    # print(text)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    try:\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                w = num2words(int(w))\n",
    "            except:\n",
    "                a = 0\n",
    "            new_text = new_text + \" \" + w\n",
    "        new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "\n",
    "    except:\n",
    "        print (\"Exception .....\")\n",
    "    return new_text\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def lemmatizing_words(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)  # remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)  # needed again as we need to stem the words\n",
    "    data = remove_punctuation(data)  # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data)  # needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    data = lemmatizing_words(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "processed_text = []\n",
    "processed_title = []\n",
    "for i in dataset[:N]:\n",
    "    # file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "\n",
    "    # text = file.read().strip()\n",
    "    text = urllib.request.urlopen(i[0]).read().decode('ISO-8859-1').strip()\n",
    "\n",
    "    # file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))\n",
    "\n",
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "\n",
    "total_vocab_size = len(DF)\n",
    "\n",
    "print (\"total_vocab_size:\", total_vocab_size)\n",
    "\n",
    "total_vocab = [x for x in DF]\n",
    "\n",
    "print(total_vocab[:20])\n",
    "\n",
    "\n",
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c\n",
    "\n",
    "\n",
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    tokens = processed_text[i]\n",
    "\n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token] / words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N + 1) / (df + 1))\n",
    "\n",
    "        tf_idf[doc, token] = tf * idf\n",
    "\n",
    "    doc += 1\n",
    "\n",
    "doc = 0\n",
    "\n",
    "tf_idf_title = {}\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_text[i])\n",
    "    words_count = len(tokens + processed_text[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token] / words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N + 1) / (df + 1))  # numerator is added 1 to avoid negative values\n",
    "\n",
    "        tf_idf_title[doc, token] = tf * idf\n",
    "\n",
    "    doc += 1\n",
    "\n",
    "print(\"tf_idf:\", tf_idf[(0, \"renaud\")])\n",
    "\n",
    "\n",
    "print(\"th_idf_title:\", tf_idf_title[(0, \"renaud\")])\n",
    "\n",
    "\n",
    "for i in tf_idf:\n",
    "    tf_idf[i] *= alpha\n",
    "\n",
    "\n",
    "for i in tf_idf_title:\n",
    "    tf_idf[i] = tf_idf_title[i]\n",
    "\n",
    "\n",
    "print(\"len tf_idf:\", len(tf_idf))\n",
    "\n",
    "\n",
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    query_weights = {}\n",
    "    for key in tf_idf:\n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\")\n",
    "    l = []\n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "\n",
    "    print (\"l:\", l)\n",
    "    # print_doc(l[0])\n",
    "    print(dataset[l[1]][0])\n",
    "    s = \"\"\n",
    "    s = dataset[l[0]][0]+\",  \"+dataset[l[1]][0]+\", \"+dataset[l[2]][0]\n",
    "    print ('**s:',s)\n",
    "    return s;\n",
    "\n",
    "\n",
    "#matching_score(10, \"Power Outage\")\n",
    "\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "\n",
    "        tf = counter[token] / words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N + 1) / (df + 1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf * idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    d_cosines = []\n",
    "\n",
    "    query_vector = gen_vector(tokens)\n",
    "\n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "\n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(out)\n",
    "\n",
    "\n",
    "\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "FAULT_EXTRACTED_DATA_TOPIC = \"fault-extracted-data-events\"\n",
    "BOT_EXTRACTED_FAULT_EVENTS_TOPIC = \"bot-extracted-fault-events\"\n",
    "BOT_RESPONSE_EVENTS_TOPIC = \"bot-fault-events-sub\"\n",
    "\n",
    "def on_message(client, userdata, message):\n",
    "    print(\"***** %s ***** %s\" % (message.topic, message.payload))\n",
    "    data = message.payload\n",
    "    x = json.loads(data, object_hook=lambda d: namedtuple('X', d.keys())(*d.values()))\n",
    "    print (x.deviceId, x.fault, x.topic)\n",
    "    if (x.topic == \"nlp-fault-events\"):\n",
    "        print(\"%%%%received message to topic nlp-fault-events and insering in database%%%\")\n",
    "        print(\"Matching simillarity called for cosine_simillarity\")\n",
    "        #Q = cosine_similarity(10, x.fault)\n",
    "        l =  matching_score(10,x.fault)\n",
    "        # printing the list using loop\n",
    "        print(\"first matched topic:\")\n",
    "        print(\"%%% l is:\",l)\n",
    "        for x in range(len(l)):\n",
    "            print_doc(l[x]),\n",
    "            print (\"##############################################################\")\n",
    "\n",
    "\n",
    "    elif (x.topic == \"bot-fault-events\"):\n",
    "        print(\"Publishing message to topic\", \"chatbot-fault-events\")\n",
    "        l = matching_score(10, x.fault)\n",
    "        # printing the list using loop\n",
    "        print(\"first matched topic:\")\n",
    "        print(\"%%% l is:\", l)\n",
    "        payloadMsg = l\n",
    "        client.publish(BOT_RESPONSE_EVENTS_TOPIC, payloadMsg, qos=2)\n",
    "        print(\"message published on bot-fault-events-sub\")\n",
    "\n",
    "def on_log(client, userdata, level, buf):\n",
    "    #print(\"log: \",buf)\n",
    "    print (\".\")\n",
    "\n",
    "############################################\n",
    "\n",
    "broker_address=\"192.168.1.3\"\n",
    "print(\"creating new instance\")\n",
    "client = mqtt.Client(\"P2\") #create new instance\n",
    "#client = mqtt.Client()\n",
    "client.on_message = on_message#attach function to callback\n",
    "client.on_log=on_log\n",
    "\n",
    "print(\"connecting to broker\")\n",
    "client.connect(broker_address, 1883, 60)#connect to broker\n",
    "\n",
    "print(\"Subscribing to topic\",\"fault-extracted-data-events\")\n",
    "message = client.subscribe(FAULT_EXTRACTED_DATA_TOPIC, qos=1)\n",
    "print(\"Subscribing to topic\",\"fault-extracted-data-events\")\n",
    "message = client.subscribe(BOT_EXTRACTED_FAULT_EVENTS_TOPIC, qos=1)\n",
    "\n",
    "print (\"Message received\")\n",
    "\n",
    "# Blocking call that processes network traffic, dispatches callbacks and\n",
    "# handles reconnecting.\n",
    "# Other loop*() functions are available that give a threaded interface and a\n",
    "# manual interface.\n",
    "client.loop_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
